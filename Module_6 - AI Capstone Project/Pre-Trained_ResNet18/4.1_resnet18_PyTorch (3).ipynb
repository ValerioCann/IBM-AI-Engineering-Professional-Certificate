{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://cocl.us/pytorch_link_top\">\n",
    "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n",
    "</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><h1>Pre-trained-Models with PyTorch </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will use pre-trained models to classify between the negative and positive samples; you will be provided with the dataset object. The particular pre-trained model will be resnet18; you will have three questions: \n",
    "<ul>\n",
    "<li>change the output layer</li>\n",
    "<li> train the model</li> \n",
    "<li>  identify  several  misclassified samples</li> \n",
    " </ul>\n",
    "You will take several screenshots of your work and share your notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#download_data\"> Download Data</a></li>\n",
    "    <li><a href=\"#auxiliary\"> Imports and Auxiliary Functions </a></li>\n",
    "    <li><a href=\"#data_class\"> Dataset Class</a></li>\n",
    "    <li><a href=\"#Question_1\">Question 1</a></li>\n",
    "    <li><a href=\"#Question_2\">Question 2</a></li>\n",
    "    <li><a href=\"#Question_3\">Question 3</a></li>\n",
    "</ul>\n",
    "<p>Estimated Time Needed: <strong>120 min</strong></p>\n",
    " </div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"download_data\">Download Data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset and unzip the files in your data directory, unlike the other labs, all the data will be deleted after you close  the lab, this may take some time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-09 15:52:06--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\n",
      "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
      "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2598656062 (2.4G) [application/zip]\n",
      "Saving to: ‘Positive_tensors.zip’\n",
      "\n",
      "Positive_tensors.zi 100%[===================>]   2.42G  16.0MB/s    in 2m 34s  \n",
      "\n",
      "2020-02-09 15:54:40 (16.1 MB/s) - ‘Positive_tensors.zip’ saved [2598656062/2598656062]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q Positive_tensors.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-09 16:10:37--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
      "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
      "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2111408108 (2.0G) [application/zip]\n",
      "Saving to: ‘Negative_tensors.zip’\n",
      "\n",
      "Negative_tensors.zi 100%[===================>]   1.97G  15.8MB/s    in 2m 8s   \n",
      "\n",
      "2020-02-09 16:12:45 (15.8 MB/s) - ‘Negative_tensors.zip’ saved [2111408108/2111408108]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
    "!unzip -q Negative_tensors.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will install torchvision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (0.2.1)\n",
      "Requirement already satisfied: numpy in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from torchvision) (1.16.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from torchvision) (6.2.1)\n",
      "Requirement already satisfied: six in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from torchvision) (1.13.0)\n",
      "Requirement already satisfied: torch in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from torchvision) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb2b41902b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the libraries will be used for this lab.\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import pandas\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import torch \n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "import os\n",
    "import glob\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pylab as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"data_class\">Dataset Class</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This dataset class is essentially the same dataset you build in the previous section, but to speed things up, we are going to use tensors instead of jpeg images. Therefor for each iteration, you will skip the reshape step, conversion step to tensors and normalization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Create your own dataset object\n",
    "\n",
    "class Dataset(Dataset):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self,transform=None,train=True):\n",
    "        directory=\"\"\n",
    "        positive=\"Positive_tensors\"\n",
    "        negative='Negative_tensors'\n",
    "\n",
    "        positive_file_path=os.path.join(directory,positive)\n",
    "        negative_file_path=os.path.join(directory,negative)\n",
    "        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n",
    "        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n",
    "        number_of_samples=len(positive_files)+len(negative_files)\n",
    "        self.all_files=[None]*number_of_samples\n",
    "        self.all_files[::2]=positive_files\n",
    "        self.all_files[1::2]=negative_files \n",
    "        # The transform is goint to be used on image\n",
    "        self.transform = transform\n",
    "        #torch.LongTensor\n",
    "        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n",
    "        self.Y[::2]=1\n",
    "        self.Y[1::2]=0\n",
    "        \n",
    "        if train:\n",
    "            self.all_files=self.all_files[0:30000]\n",
    "            self.Y=self.Y[0:30000]\n",
    "            self.len=len(self.all_files)\n",
    "        else:\n",
    "            self.all_files=self.all_files[30000:]\n",
    "            self.Y=self.Y[30000:]\n",
    "            self.len=len(self.all_files)     \n",
    "       \n",
    "    # Get the length\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    # Getter\n",
    "    def __getitem__(self, idx):\n",
    "               \n",
    "        image=torch.load(self.all_files[idx])\n",
    "        y=self.Y[idx]\n",
    "                  \n",
    "        # If there is any transform method, apply it onto the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, y\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create two dataset objects, one for the training data and one for the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "30000\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(train=True)\n",
    "validation_dataset = Dataset(train=False)\n",
    "print(\"done\")\n",
    "print(len(train_dataset))\n",
    "print(train_dataset[0][0].shape)\n",
    "print(validation_dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Question_1\">Question 1</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Prepare a pre-trained resnet18 model :</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the pre-trained model resnet18\n",
    "\n",
    "# Type your code here\n",
    "\n",
    "model_resnet18 = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Set the parameter cannot be trained for the pre-trained model\n",
    "\n",
    "\n",
    "# Type your code here\n",
    "\n",
    "for param in model_resnet18.parameters():\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet18.fc = nn.Linear(512, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_resnet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Question_2\">Question 2: Train the Model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question you will train your, model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 1</b>: Create a cross entropy criterion function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create the loss function\n",
    "\n",
    "# Type your code here\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = train_dataset, batch_size = 100)\n",
    "validation_loader = DataLoader(dataset = validation_dataset, batch_size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 3</b>: Use the following optimizer to minimize the loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([parameters  for parameters in model_resnet18.parameters() if parameters.requires_grad],lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete the following code to calculate  the accuracy on the validation data for one epoch; this should take about 45 minutes. Make sure you calculate the accuracy on the validation data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0) Loss: 0.6651895046234131\n",
      "1) Loss: 0.6660417914390564\n",
      "2) Loss: 0.6665399074554443\n",
      "3) Loss: 0.5984556078910828\n",
      "4) Loss: 0.5450731515884399\n",
      "5) Loss: 0.5304229855537415\n",
      "6) Loss: 0.47249332070350647\n",
      "7) Loss: 0.48025986552238464\n",
      "8) Loss: 0.4503706693649292\n",
      "9) Loss: 0.4012661874294281\n",
      "10) Loss: 0.4070867598056793\n",
      "11) Loss: 0.38338223099708557\n",
      "12) Loss: 0.3592695891857147\n",
      "13) Loss: 0.32072558999061584\n",
      "14) Loss: 0.29306426644325256\n",
      "15) Loss: 0.27054983377456665\n",
      "16) Loss: 0.29023003578186035\n",
      "17) Loss: 0.2698443531990051\n",
      "18) Loss: 0.2537205219268799\n",
      "19) Loss: 0.2506779730319977\n",
      "20) Loss: 0.23578323423862457\n",
      "21) Loss: 0.1806291937828064\n",
      "22) Loss: 0.23531833291053772\n",
      "23) Loss: 0.18423058092594147\n",
      "24) Loss: 0.19387947022914886\n",
      "25) Loss: 0.19448499381542206\n",
      "26) Loss: 0.1823401302099228\n",
      "27) Loss: 0.18532346189022064\n",
      "28) Loss: 0.18144546449184418\n",
      "29) Loss: 0.17961879074573517\n",
      "30) Loss: 0.18955759704113007\n",
      "31) Loss: 0.15055693686008453\n",
      "32) Loss: 0.15007050335407257\n",
      "33) Loss: 0.12859176099300385\n",
      "34) Loss: 0.16167192161083221\n",
      "35) Loss: 0.13732877373695374\n",
      "36) Loss: 0.12117817252874374\n",
      "37) Loss: 0.11484728753566742\n",
      "38) Loss: 0.1273851990699768\n",
      "39) Loss: 0.11429475992918015\n",
      "40) Loss: 0.12127824127674103\n",
      "41) Loss: 0.11187417805194855\n",
      "42) Loss: 0.09239646047353745\n",
      "43) Loss: 0.09963174164295197\n",
      "44) Loss: 0.11917153000831604\n",
      "45) Loss: 0.1096445769071579\n",
      "46) Loss: 0.09199491143226624\n",
      "47) Loss: 0.11541292816400528\n",
      "48) Loss: 0.1081276684999466\n",
      "49) Loss: 0.10071203857660294\n",
      "50) Loss: 0.12002675235271454\n",
      "51) Loss: 0.11291296035051346\n",
      "52) Loss: 0.08732308447360992\n",
      "53) Loss: 0.07404399663209915\n",
      "54) Loss: 0.10497260093688965\n",
      "55) Loss: 0.11446380615234375\n",
      "56) Loss: 0.08948684483766556\n",
      "57) Loss: 0.10217132419347763\n",
      "58) Loss: 0.07653727382421494\n",
      "59) Loss: 0.0906098261475563\n",
      "60) Loss: 0.08140911906957626\n",
      "61) Loss: 0.0825309306383133\n",
      "62) Loss: 0.06424565613269806\n",
      "63) Loss: 0.08463028073310852\n",
      "64) Loss: 0.09431865811347961\n",
      "65) Loss: 0.10944261401891708\n",
      "66) Loss: 0.06841336935758591\n",
      "67) Loss: 0.10050450265407562\n",
      "68) Loss: 0.07569466531276703\n",
      "69) Loss: 0.07136929780244827\n",
      "70) Loss: 0.07315213978290558\n",
      "71) Loss: 0.0970955565571785\n",
      "72) Loss: 0.06253449618816376\n",
      "73) Loss: 0.09615376591682434\n",
      "74) Loss: 0.10147611796855927\n",
      "75) Loss: 0.08524682372808456\n",
      "76) Loss: 0.08285480737686157\n",
      "77) Loss: 0.0654204860329628\n",
      "78) Loss: 0.06495023518800735\n",
      "79) Loss: 0.08740754425525665\n",
      "80) Loss: 0.06331641972064972\n",
      "81) Loss: 0.06201796606183052\n",
      "82) Loss: 0.060480810701847076\n",
      "83) Loss: 0.05092494189739227\n",
      "84) Loss: 0.06406388431787491\n",
      "85) Loss: 0.06059207022190094\n",
      "86) Loss: 0.05625615268945694\n",
      "87) Loss: 0.06037281081080437\n",
      "88) Loss: 0.06337601691484451\n",
      "89) Loss: 0.055740442126989365\n",
      "90) Loss: 0.07029418647289276\n",
      "91) Loss: 0.0558769553899765\n",
      "92) Loss: 0.07589681446552277\n",
      "93) Loss: 0.0565231516957283\n",
      "94) Loss: 0.08592068403959274\n",
      "95) Loss: 0.05831115320324898\n",
      "96) Loss: 0.057560909539461136\n",
      "97) Loss: 0.051765117794275284\n",
      "98) Loss: 0.056522056460380554\n",
      "99) Loss: 0.06485699117183685\n",
      "100) Loss: 0.0484015978872776\n",
      "101) Loss: 0.058190152049064636\n",
      "102) Loss: 0.06313904374837875\n",
      "103) Loss: 0.06443388760089874\n",
      "104) Loss: 0.03887547552585602\n",
      "105) Loss: 0.04023602977395058\n",
      "106) Loss: 0.05658544972538948\n",
      "107) Loss: 0.045797694474458694\n",
      "108) Loss: 0.04648840054869652\n",
      "109) Loss: 0.051728032529354095\n",
      "113) Loss: 0.06280157715082169\n",
      "114) Loss: 0.035710807889699936\n",
      "115) Loss: 0.0587024912238121\n",
      "116) Loss: 0.04688362777233124\n",
      "117) Loss: 0.04887421429157257\n",
      "118) Loss: 0.07465006411075592\n",
      "119) Loss: 0.05598054826259613\n",
      "120) Loss: 0.06329486519098282\n",
      "121) Loss: 0.0314905159175396\n",
      "122) Loss: 0.052258677780628204\n",
      "123) Loss: 0.046314727514982224\n",
      "124) Loss: 0.06641900539398193\n",
      "125) Loss: 0.05960896611213684\n",
      "126) Loss: 0.055037517100572586\n",
      "127) Loss: 0.04781892150640488\n",
      "128) Loss: 0.050167642533779144\n",
      "129) Loss: 0.035645704716444016\n",
      "130) Loss: 0.05480358004570007\n",
      "131) Loss: 0.08214841037988663\n",
      "132) Loss: 0.06834321469068527\n",
      "133) Loss: 0.04485510662198067\n",
      "134) Loss: 0.03840605169534683\n",
      "135) Loss: 0.04680432006716728\n",
      "136) Loss: 0.04274797812104225\n",
      "137) Loss: 0.04610716924071312\n",
      "138) Loss: 0.04381532967090607\n",
      "139) Loss: 0.0608929879963398\n",
      "140) Loss: 0.03812327980995178\n",
      "141) Loss: 0.033990778028964996\n",
      "142) Loss: 0.04016568511724472\n",
      "143) Loss: 0.04214271530508995\n",
      "144) Loss: 0.09525933861732483\n",
      "145) Loss: 0.03505457937717438\n",
      "146) Loss: 0.03728950023651123\n",
      "147) Loss: 0.07257625460624695\n",
      "148) Loss: 0.04619478061795235\n",
      "149) Loss: 0.05275804549455643\n",
      "150) Loss: 0.029648859053850174\n",
      "151) Loss: 0.04075291007757187\n",
      "152) Loss: 0.026979370042681694\n",
      "153) Loss: 0.055880408734083176\n",
      "154) Loss: 0.032013893127441406\n",
      "155) Loss: 0.05832316353917122\n",
      "156) Loss: 0.06516888737678528\n",
      "157) Loss: 0.030793029814958572\n",
      "158) Loss: 0.07107372581958771\n",
      "159) Loss: 0.03404879570007324\n",
      "160) Loss: 0.046303052455186844\n",
      "161) Loss: 0.06617184728384018\n",
      "162) Loss: 0.036779314279556274\n",
      "163) Loss: 0.0341438427567482\n",
      "164) Loss: 0.04459851235151291\n",
      "165) Loss: 0.04905204847455025\n",
      "166) Loss: 0.030020758509635925\n",
      "167) Loss: 0.036070793867111206\n",
      "168) Loss: 0.05415046215057373\n",
      "169) Loss: 0.030971620231866837\n",
      "170) Loss: 0.11602197587490082\n",
      "171) Loss: 0.032779812812805176\n",
      "172) Loss: 0.05530431121587753\n",
      "173) Loss: 0.0462682731449604\n",
      "174) Loss: 0.04666677117347717\n",
      "175) Loss: 0.04852661117911339\n",
      "176) Loss: 0.03182423859834671\n",
      "177) Loss: 0.042864616960287094\n",
      "178) Loss: 0.06725508719682693\n",
      "179) Loss: 0.03437527269124985\n",
      "180) Loss: 0.05840083956718445\n",
      "181) Loss: 0.03781404718756676\n",
      "182) Loss: 0.03557467460632324\n",
      "183) Loss: 0.022785473614931107\n",
      "184) Loss: 0.025257473811507225\n",
      "185) Loss: 0.03208437189459801\n",
      "186) Loss: 0.07180102169513702\n",
      "187) Loss: 0.03724180534482002\n",
      "188) Loss: 0.0406874381005764\n",
      "189) Loss: 0.03439797833561897\n",
      "190) Loss: 0.039565686136484146\n",
      "191) Loss: 0.0452948659658432\n",
      "192) Loss: 0.021715085953474045\n",
      "193) Loss: 0.03151987865567207\n",
      "194) Loss: 0.0421968474984169\n",
      "195) Loss: 0.01990576833486557\n",
      "196) Loss: 0.03595883771777153\n",
      "197) Loss: 0.034141264855861664\n",
      "198) Loss: 0.04510551318526268\n",
      "199) Loss: 0.043291639536619186\n",
      "200) Loss: 0.030372055247426033\n",
      "201) Loss: 0.04676332324743271\n",
      "202) Loss: 0.02932088077068329\n",
      "203) Loss: 0.02103651687502861\n",
      "204) Loss: 0.02573084831237793\n",
      "205) Loss: 0.028099291026592255\n",
      "206) Loss: 0.02579585276544094\n",
      "207) Loss: 0.018079673871397972\n",
      "208) Loss: 0.03715149313211441\n",
      "209) Loss: 0.041997622698545456\n",
      "210) Loss: 0.05144093185663223\n",
      "211) Loss: 0.024716677144169807\n",
      "212) Loss: 0.028667477890849113\n",
      "213) Loss: 0.04335213080048561\n",
      "214) Loss: 0.06525028496980667\n",
      "215) Loss: 0.039421197026968\n",
      "216) Loss: 0.06282990425825119\n",
      "217) Loss: 0.03917401656508446\n",
      "218) Loss: 0.03673330321907997\n",
      "219) Loss: 0.03502185642719269\n",
      "220) Loss: 0.03611751273274422\n",
      "221) Loss: 0.03672443702816963\n",
      "222) Loss: 0.036907441914081573\n",
      "223) Loss: 0.029427535831928253\n",
      "224) Loss: 0.024764658883213997\n",
      "225) Loss: 0.025039343163371086\n",
      "226) Loss: 0.025735482573509216\n",
      "227) Loss: 0.02598806843161583\n",
      "228) Loss: 0.025058362632989883\n",
      "229) Loss: 0.028583252802491188\n",
      "230) Loss: 0.01945890672504902\n",
      "231) Loss: 0.017651835456490517\n",
      "232) Loss: 0.04163026437163353\n",
      "233) Loss: 0.035869255661964417\n",
      "234) Loss: 0.024212388321757317\n",
      "235) Loss: 0.023005519062280655\n",
      "236) Loss: 0.023110294714570045\n",
      "237) Loss: 0.04193811118602753\n",
      "238) Loss: 0.030619602650403976\n",
      "239) Loss: 0.036088377237319946\n",
      "240) Loss: 0.06081349030137062\n",
      "241) Loss: 0.02873018942773342\n",
      "242) Loss: 0.033056315034627914\n",
      "243) Loss: 0.029067108407616615\n",
      "244) Loss: 0.03840410336852074\n",
      "245) Loss: 0.031098879873752594\n",
      "246) Loss: 0.04548080265522003\n",
      "247) Loss: 0.02866273932158947\n",
      "248) Loss: 0.03921438008546829\n",
      "249) Loss: 0.02712613344192505\n",
      "250) Loss: 0.038644939661026\n",
      "251) Loss: 0.037382274866104126\n",
      "252) Loss: 0.029302477836608887\n",
      "253) Loss: 0.02585822530090809\n",
      "254) Loss: 0.021610498428344727\n",
      "255) Loss: 0.04986323416233063\n",
      "256) Loss: 0.02046831138432026\n",
      "257) Loss: 0.036100614815950394\n",
      "258) Loss: 0.0376424603164196\n",
      "259) Loss: 0.061074186116456985\n",
      "260) Loss: 0.035465460270643234\n",
      "261) Loss: 0.03723585978150368\n",
      "262) Loss: 0.035621095448732376\n",
      "263) Loss: 0.01624944619834423\n",
      "264) Loss: 0.020225506275892258\n",
      "265) Loss: 0.02756459079682827\n",
      "266) Loss: 0.029056571424007416\n",
      "267) Loss: 0.027285143733024597\n",
      "268) Loss: 0.011103455908596516\n",
      "269) Loss: 0.016918882727622986\n",
      "270) Loss: 0.026330608874559402\n",
      "271) Loss: 0.025894591584801674\n",
      "272) Loss: 0.020973946899175644\n",
      "273) Loss: 0.032849833369255066\n",
      "274) Loss: 0.03859122842550278\n",
      "275) Loss: 0.014818660914897919\n",
      "276) Loss: 0.030798407271504402\n",
      "277) Loss: 0.018572192639112473\n",
      "278) Loss: 0.013757565058767796\n",
      "279) Loss: 0.05064579099416733\n",
      "280) Loss: 0.030726034194231033\n",
      "281) Loss: 0.027669522911310196\n",
      "282) Loss: 0.023045388981699944\n",
      "283) Loss: 0.030115356668829918\n",
      "284) Loss: 0.04925374686717987\n",
      "285) Loss: 0.025061337277293205\n",
      "286) Loss: 0.02425047941505909\n",
      "287) Loss: 0.022685525938868523\n",
      "288) Loss: 0.02180895023047924\n",
      "289) Loss: 0.034810274839401245\n",
      "290) Loss: 0.028187766671180725\n",
      "291) Loss: 0.016840586438775063\n",
      "292) Loss: 0.0185539573431015\n",
      "293) Loss: 0.03135525807738304\n",
      "294) Loss: 0.025787312537431717\n",
      "295) Loss: 0.021050702780485153\n",
      "296) Loss: 0.016608715057373047\n",
      "297) Loss: 0.03303946554660797\n",
      "298) Loss: 0.026875685900449753\n",
      "299) Loss: 0.02711757831275463\n",
      "0) accuracy: 0.01\n",
      "1) accuracy: 0.0198\n",
      "2) accuracy: 0.0298\n",
      "3) accuracy: 0.0398\n",
      "4) accuracy: 0.0496\n",
      "5) accuracy: 0.0596\n",
      "6) accuracy: 0.0696\n",
      "7) accuracy: 0.0796\n",
      "8) accuracy: 0.0895\n",
      "9) accuracy: 0.0994\n",
      "10) accuracy: 0.1092\n",
      "11) accuracy: 0.1191\n",
      "12) accuracy: 0.129\n",
      "13) accuracy: 0.139\n",
      "14) accuracy: 0.149\n",
      "15) accuracy: 0.1589\n",
      "16) accuracy: 0.1689\n",
      "17) accuracy: 0.1788\n",
      "18) accuracy: 0.1888\n",
      "19) accuracy: 0.1988\n",
      "20) accuracy: 0.2087\n",
      "21) accuracy: 0.2187\n",
      "22) accuracy: 0.2286\n",
      "23) accuracy: 0.2386\n",
      "24) accuracy: 0.2486\n",
      "25) accuracy: 0.2585\n",
      "26) accuracy: 0.2684\n",
      "27) accuracy: 0.2784\n",
      "28) accuracy: 0.2884\n",
      "29) accuracy: 0.2983\n",
      "30) accuracy: 0.3083\n",
      "31) accuracy: 0.3183\n",
      "32) accuracy: 0.3283\n",
      "33) accuracy: 0.3383\n",
      "34) accuracy: 0.3482\n",
      "35) accuracy: 0.3581\n",
      "36) accuracy: 0.368\n",
      "37) accuracy: 0.3779\n",
      "38) accuracy: 0.3878\n",
      "39) accuracy: 0.3977\n",
      "40) accuracy: 0.4076\n",
      "41) accuracy: 0.4175\n",
      "42) accuracy: 0.4275\n",
      "43) accuracy: 0.4374\n",
      "44) accuracy: 0.4474\n",
      "45) accuracy: 0.4574\n",
      "46) accuracy: 0.4671\n",
      "47) accuracy: 0.4771\n",
      "48) accuracy: 0.4871\n",
      "49) accuracy: 0.4971\n",
      "50) accuracy: 0.5071\n",
      "51) accuracy: 0.5171\n",
      "52) accuracy: 0.527\n",
      "53) accuracy: 0.537\n",
      "54) accuracy: 0.5468\n",
      "55) accuracy: 0.5568\n",
      "56) accuracy: 0.5668\n",
      "57) accuracy: 0.5767\n",
      "58) accuracy: 0.5866\n",
      "59) accuracy: 0.5966\n",
      "60) accuracy: 0.6066\n",
      "61) accuracy: 0.6166\n",
      "62) accuracy: 0.6266\n",
      "63) accuracy: 0.6366\n",
      "64) accuracy: 0.6465\n",
      "65) accuracy: 0.6565\n",
      "66) accuracy: 0.6665\n",
      "67) accuracy: 0.6764\n",
      "68) accuracy: 0.6862\n",
      "69) accuracy: 0.6962\n",
      "70) accuracy: 0.7062\n",
      "71) accuracy: 0.7162\n",
      "72) accuracy: 0.7262\n",
      "73) accuracy: 0.7362\n",
      "74) accuracy: 0.7461\n",
      "75) accuracy: 0.756\n",
      "76) accuracy: 0.7659\n",
      "77) accuracy: 0.7757\n",
      "78) accuracy: 0.7856\n",
      "79) accuracy: 0.7956\n",
      "80) accuracy: 0.8054\n",
      "81) accuracy: 0.8153\n",
      "82) accuracy: 0.8251\n",
      "83) accuracy: 0.835\n",
      "84) accuracy: 0.845\n",
      "85) accuracy: 0.855\n",
      "86) accuracy: 0.865\n",
      "87) accuracy: 0.875\n",
      "88) accuracy: 0.885\n",
      "89) accuracy: 0.895\n",
      "90) accuracy: 0.905\n",
      "91) accuracy: 0.915\n",
      "92) accuracy: 0.9249\n",
      "93) accuracy: 0.9349\n",
      "94) accuracy: 0.9448\n",
      "95) accuracy: 0.9548\n",
      "96) accuracy: 0.9648\n",
      "97) accuracy: 0.9748\n",
      "98) accuracy: 0.9848\n",
      "99) accuracy: 0.9948\n",
      "DONE DONE DONE\n"
     ]
    }
   ],
   "source": [
    "n_epochs=1\n",
    "loss_list=[]\n",
    "accuracy_list=[]\n",
    "correct=0\n",
    "N_test=len(validation_dataset)\n",
    "N_train=len(train_dataset)\n",
    "start_time = time.time()\n",
    "#n_epochs\n",
    "\n",
    "Loss=0\n",
    "start_time = time.time()\n",
    "count=0\n",
    "\n",
    "#useful_stuff = {'training_loss':[], 'validation_accuracy':[]}  \n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for x, y in train_loader:\n",
    "        \n",
    "        model_resnet18.train() \n",
    "        #clear gradient \n",
    "        optimizer.zero_grad()\n",
    "        #make a prediction\n",
    "        #z = model_resnet18(x.view(-1, 3, 224, 224))\n",
    "        z = model_resnet18(x)\n",
    "        # calculate loss\n",
    "        loss = criterion(z, y)\n",
    "        # calculate gradients of parameters \n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_list.append(loss.data)\n",
    "        #useful_stuff['training_loss'].append(loss.item())\n",
    "        print(str(count)+') Loss: '+str(loss.item()))\n",
    "        count+=1\n",
    "    \n",
    "    count=0\n",
    "    correct=0\n",
    "    for xt, yt in validation_loader:\n",
    "        # set model to eval \n",
    "        model_resnet18.eval() \n",
    "        #make a prediction \n",
    "        #zt = model_resnet18(xt.view(-1, 3, 224, 224))\n",
    "        zt = model_resnet18(xt)\n",
    "        #find max \n",
    "        _, label=torch.max(zt, 1)\n",
    "        #Calculate misclassified  samples in mini-batch \n",
    "        #hint +=(yhat==y_test).sum().item()\n",
    "        correct += (label == yt).sum().item()\n",
    "        accuracytemp = 100 * (correct / len(validation_dataset))\n",
    "        accuracytemp=correct/N_test\n",
    "        print(str(count)+') accuracy: '+str(accuracytemp))\n",
    "        #useful_stuff['validation_accuracy'].append(accuracy)\n",
    "        count+=1\n",
    "        \n",
    "    accuracy = 100 * (correct / len(validation_dataset))\n",
    "    accuracy=correct/N_test\n",
    "\n",
    "print('DONE DONE DONE')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Print out the Accuracy and plot the loss stored in the list <code>loss_list</code> for every iteration and take a screen shot.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9948"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list)\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Question_3\">Question 3:Find the misclassified samples</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Identify the first four misclassified samples using the validation data:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) indx: 146 predicted value: tensor(0) real value: tensor(1)\n",
      "2) indx: 164 predicted value: tensor(0) real value: tensor(1)\n",
      "3) indx: 456 predicted value: tensor(0) real value: tensor(1)\n",
      "4) indx: 480 predicted value: tensor(0) real value: tensor(1)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "batchcount=0\n",
    "incorrect=0\n",
    "for x, y in validation_loader:\n",
    "    if (count<4):\n",
    "        # set model to eval \n",
    "        model_resnet18.eval() \n",
    "       \n",
    "        #make a prediction \n",
    "        z = model_resnet18(x.view(-1, 3, 224, 224))\n",
    "        \n",
    "        #find max \n",
    "        _, label=torch.max(z, 1)\n",
    "       \n",
    "        #Calculate misclassified  samples in mini-batch \n",
    "        #hint +=(yhat==y_test).sum().item()\n",
    "        correct += (label == y).sum().item()\n",
    "        incorrect += (label != y).sum().item()\n",
    "        #print(label)\n",
    "        #print(y)\n",
    "        #print(correct)\n",
    "        #print('total # of missclassified samples :'+str(incorrect))\n",
    "        for i in range(0,100):\n",
    "            if (label[i]!=y[i]):\n",
    "                count += 1\n",
    "                print(str(count)+') indx: '+str(batchcount+i),'predicted value: '+str(label[i])+' real value: '+str(y[i]))\n",
    "        batchcount+=100\n",
    "    else:\n",
    "        print(\"done\")\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/share-notebooks.html\"> CLICK HERE </a> Click here to see how to share your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>About the Authors:</h2> \n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2018 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
